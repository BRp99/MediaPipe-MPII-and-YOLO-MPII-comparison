{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37c3591-e1f7-4cd4-a39f-32a1952f0d8d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79924e74-5b0a-40af-9d6b-ba5635b5d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b5203f-dc5b-43bf-aeb3-0440dcc19c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000111209.jpg: 480x640 1 person, 842.6ms\n",
      "Speed: 3.0ms preprocess, 842.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000114595.jpg: 384x640 2 persons, 688.7ms\n",
      "Speed: 32.4ms preprocess, 688.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000120361.jpg: 480x640 11 persons, 870.7ms\n",
      "Speed: 0.0ms preprocess, 870.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000124344.jpg: 384x640 10 persons, 612.2ms\n",
      "Speed: 0.0ms preprocess, 612.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000128443.jpg: 384x640 1 person, 644.6ms\n",
      "Speed: 3.0ms preprocess, 644.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000131987.jpg: 480x640 1 person, 812.9ms\n",
      "Speed: 4.9ms preprocess, 812.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000133364.jpg: 480x640 7 persons, 737.4ms\n",
      "Speed: 4.0ms preprocess, 737.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000135363.jpg: 480x640 3 persons, 850.6ms\n",
      "Speed: 6.6ms preprocess, 850.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000141822.jpg: 384x640 1 person, 749.0ms\n",
      "Speed: 0.0ms preprocess, 749.0ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\MediaPipe-MPII-and-YOLO-MPII-comparison\\mpii\\images\\000142573.jpg: 384x640 1 person, 692.0ms\n",
      "Speed: 0.0ms preprocess, 692.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Keypoints' object has no attribute 'coordenates_x_y_n'. See valid attributes below.\n\n    A class for storing and manipulating detection keypoints.\n\n    Attributes:\n        xy (torch.Tensor): A collection of keypoints containing x, y coordinates for each detection.\n        xyn (torch.Tensor): A normalized version of xy with coordinates in the range [0, 1].\n        conf (torch.Tensor): Confidence values associated with keypoints if available, otherwise None.\n\n    Methods:\n        cpu(): Returns a copy of the keypoints tensor on CPU memory.\n        numpy(): Returns a copy of the keypoints tensor as a numpy array.\n        cuda(): Returns a copy of the keypoints tensor on GPU memory.\n        to(device, dtype): Returns a copy of the keypoints tensor with the specified device and dtype.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m         keypoints \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeypoints\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m#print(\"predicted Keypoints:\", keypoints)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         coordenates_x_y_n\u001b[38;5;241m=\u001b[39m \u001b[43mkeypoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordenates_x_y_n\u001b[49m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordenates x, y normalized:\u001b[39m\u001b[38;5;124m\"\u001b[39m, coordenates_x_y_n)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\ultralytics\\utils\\__init__.py:156\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Keypoints' object has no attribute 'coordenates_x_y_n'. See valid attributes below.\n\n    A class for storing and manipulating detection keypoints.\n\n    Attributes:\n        xy (torch.Tensor): A collection of keypoints containing x, y coordinates for each detection.\n        xyn (torch.Tensor): A normalized version of xy with coordinates in the range [0, 1].\n        conf (torch.Tensor): Confidence values associated with keypoints if available, otherwise None.\n\n    Methods:\n        cpu(): Returns a copy of the keypoints tensor on CPU memory.\n        numpy(): Returns a copy of the keypoints tensor as a numpy array.\n        cuda(): Returns a copy of the keypoints tensor on GPU memory.\n        to(device, dtype): Returns a copy of the keypoints tensor with the specified device and dtype.\n    "
     ]
    }
   ],
   "source": [
    "# load model YOLO v8\n",
    "model = YOLO(\"yolov8m-pose.pt\")\n",
    "\n",
    "# configurations\n",
    "N = 10\n",
    "MPII_IMAGES_DIR = \"./mpii/images\"\n",
    "#IMAGE_FILES = [os.path.join(MPII_IMAGES_DIR, file) for file in os.listdir(MPII_IMAGES_DIR) if file.endswith(('.jpg'))][:N]\n",
    "yolo_images_dir = \"./YOLO_images\"\n",
    "\n",
    "count = 0\n",
    "results_list = []\n",
    "\n",
    "\n",
    "for file in os.listdir(MPII_IMAGES_DIR):\n",
    "    if file.endswith('.jpg'):\n",
    "        image_file = os.path.join(MPII_IMAGES_DIR, file)\n",
    "        shutil.copy(image_file, yolo_images_dir)\n",
    "        \n",
    "        # execute detection for YOLO_images directory\n",
    "        results = model(source=image_file, show=False, conf=0.3, save=False)\n",
    "        results_list.append(results)\n",
    "        #print(\"RESULTS LIST\", results_list)\n",
    "        count += 1\n",
    "        if count >= N:\n",
    "            break\n",
    "# acess and print predictid keypoints\n",
    "for result in results_list:\n",
    "    for result in results:\n",
    "        keypoints = result.keypoints\n",
    "        #print(\"predicted Keypoints:\", keypoints)\n",
    "        xyn = keypoints.xyn\n",
    "        print(\"Coordenadas x, y normalizadas:\", xyn)\n",
    "\n",
    "\n",
    "del model\n",
    "\n",
    "\n",
    "\n",
    "# USE ANNOTATION MPII\n",
    "#annotations_data = loadmat(\"./mpii/annotations/mpii_human_pose.mat\")\n",
    "\n",
    "#nnolist_test = annotations_data['RELEASE'] ['annolist']\n",
    "#print(\"ANOOOOOO\", annotations_data)\n",
    "#print(\"Data Structure annolist test\", annolist_test)\n",
    "# annolist_test  two-dimensional array of structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95dcc3-7f69-4a01-a873-388c606f4ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
